{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist #to import our dataset\n",
    "from tensorflow.keras.models import Sequential, Model # imports our type of network\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, BatchNormalization # imports our layers we want to use\n",
    "\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy #loss function\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD #optimisers\n",
    "from tensorflow.keras.utils import to_categorical #some function for data preparation\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #checkpoints used to keep track of best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               157000    \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 199210 (778.16 KB)\n",
      "Trainable params: 199210 (778.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28))\n",
    "x = Flatten()(input_network)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "y = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model= Model(input_network,outputs=y)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0130 - accuracy: 0.9145 - val_loss: 0.0067 - val_accuracy: 0.9570\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9637 - val_loss: 0.0049 - val_accuracy: 0.9693\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9749 - val_loss: 0.0040 - val_accuracy: 0.9740\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9805 - val_loss: 0.0039 - val_accuracy: 0.9737\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9844 - val_loss: 0.0039 - val_accuracy: 0.9743\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9872 - val_loss: 0.0035 - val_accuracy: 0.9780\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9884 - val_loss: 0.0036 - val_accuracy: 0.9763\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9902 - val_loss: 0.0036 - val_accuracy: 0.9767\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9904 - val_loss: 0.0040 - val_accuracy: 0.9751\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9925 - val_loss: 0.0030 - val_accuracy: 0.9809\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9924 - val_loss: 0.0032 - val_accuracy: 0.9795\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9934 - val_loss: 0.0032 - val_accuracy: 0.9804\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9936 - val_loss: 0.0036 - val_accuracy: 0.9772\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9935 - val_loss: 0.0032 - val_accuracy: 0.9808\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.1511e-04 - accuracy: 0.9952 - val_loss: 0.0030 - val_accuracy: 0.9821\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 9.0874e-04 - accuracy: 0.9945 - val_loss: 0.0034 - val_accuracy: 0.9784\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.7492e-04 - accuracy: 0.9947 - val_loss: 0.0031 - val_accuracy: 0.9807\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.7883e-04 - accuracy: 0.9954 - val_loss: 0.0032 - val_accuracy: 0.9808\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.6787e-04 - accuracy: 0.9954 - val_loss: 0.0033 - val_accuracy: 0.9793\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.9147e-04 - accuracy: 0.9953 - val_loss: 0.0032 - val_accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "history1=model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 200)               157000    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 199210 (778.16 KB)\n",
      "Trainable params: 199210 (778.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's add some regularisation:\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_network = Input(shape=(28,28))\n",
    "x = Flatten()(input_network)\n",
    "x = Dense(200, activation='relu',activity_regularizer=l1(0.1))(x)\n",
    "x = Dense(200, activation='relu',activity_regularizer=l1(0.2))(x)\n",
    "y = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model= Model(input_network,outputs=y)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 200)               157000    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 199210 (778.16 KB)\n",
      "Trainable params: 199210 (778.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_network = Input(shape=(28,28))\n",
    "x = Flatten()(input_network)\n",
    "x = Dense(200, activation='relu',activity_regularizer=l1(0.1))(x)\n",
    "x = Dropout(rate=dropout_rate)(x)\n",
    "x = Dense(200, activation='relu',activity_regularizer=l1(0.2))(x)\n",
    "x = Dropout(rate=dropout_rate)(x)\n",
    "y = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model_dropout= Model(input_network,outputs=y)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model_dropout.compile(loss='mse',optimizer=opt, metrics=['accuracy'])\n",
    "model_dropout.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 200)               157000    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 200)               800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200010 (781.29 KB)\n",
      "Trainable params: 199610 (779.73 KB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Batch-Normalisation\n",
    "\n",
    "dropout_rate = 0.002\n",
    "\n",
    "input_network = Input(shape=(28,28))\n",
    "x = Flatten()(input_network)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(rate=dropout_rate)(x)\n",
    "y = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model_batch_normalisation= Model(input_network,outputs=y)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model_batch_normalisation.compile(loss='mse',optimizer=opt, metrics=['accuracy'])\n",
    "model_batch_normalisation.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9307 - val_loss: 0.0059 - val_accuracy: 0.9604\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9723 - val_loss: 0.0045 - val_accuracy: 0.9713\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0034 - accuracy: 0.9787 - val_loss: 0.0038 - val_accuracy: 0.9745\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 0.9846 - val_loss: 0.0038 - val_accuracy: 0.9750\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9869 - val_loss: 0.0036 - val_accuracy: 0.9762\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0019 - accuracy: 0.9883 - val_loss: 0.0033 - val_accuracy: 0.9794\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 0.9901 - val_loss: 0.0033 - val_accuracy: 0.9788\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 0.9904 - val_loss: 0.0037 - val_accuracy: 0.9760\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9910 - val_loss: 0.0035 - val_accuracy: 0.9774\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0013 - accuracy: 0.9922 - val_loss: 0.0037 - val_accuracy: 0.9767\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9930 - val_loss: 0.0038 - val_accuracy: 0.9755\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0011 - accuracy: 0.9931 - val_loss: 0.0035 - val_accuracy: 0.9790\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 9.8340e-04 - accuracy: 0.9939 - val_loss: 0.0031 - val_accuracy: 0.9806\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.9982e-04 - accuracy: 0.9945 - val_loss: 0.0033 - val_accuracy: 0.9794\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.8607e-04 - accuracy: 0.9946 - val_loss: 0.0039 - val_accuracy: 0.9755\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.7836e-04 - accuracy: 0.9954 - val_loss: 0.0034 - val_accuracy: 0.9788\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.9711e-04 - accuracy: 0.9952 - val_loss: 0.0033 - val_accuracy: 0.9804\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.4587e-04 - accuracy: 0.9945 - val_loss: 0.0030 - val_accuracy: 0.9818\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.1890e-04 - accuracy: 0.9950 - val_loss: 0.0037 - val_accuracy: 0.9778\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.2241e-04 - accuracy: 0.9956 - val_loss: 0.0034 - val_accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "history4=model_batch_normalisation.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a model\n",
    "\n",
    "# Save the model\n",
    "#model_batch_normalisation.save('mnist_model_batch_normalisation.keras')\n",
    "\n",
    "\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('mnist_model_batch_normalisation.keras')\n",
    "#print(\"Model loaded from 'mnist_model_batch_normalisation.keras\")\n",
    "\n",
    "# Evaluate the loaded model\n",
    "#loss, accuracy = loaded_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the best model on the test/validation data.\n",
    "\n",
    "# Define a ModelCheckpoint callback\n",
    "#checkpoint = ModelCheckpoint('best_model.keras', \n",
    "#                             monitor='val_accuracy',  # You can monitor 'val_loss' or any other metric\n",
    "#                             save_best_only=True, \n",
    " #                            mode='max',  # Use 'max' if monitoring accuracy; 'min' if monitoring loss\n",
    "#                             verbose=1)\n",
    "\n",
    "#history4=model_batch_normalisation.fit(x_train, y_train,\n",
    " #         batch_size=batch_size,\n",
    " #         epochs=epochs,\n",
    " #         verbose=1,\n",
    " #         callbacks=[checkpoint],\n",
    "  #        validation_data=(x_test, y_test))\n",
    "\n",
    "#best_model = tf.keras.models.load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:36:55,195] A new study created in memory with name: no-name-6459e751-3d93-462e-be3e-9e83ab6a99c7\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:36:57,594] Trial 0 finished with value: 0.9355000257492065 and parameters: {'num_layers': 1, 'activation': 'relu', 'dropout_rate': 0.3794008514995079, 'units': 77, 'learning_rate': 0.00025401499644562805}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:00,843] Trial 1 finished with value: 0.8758999705314636 and parameters: {'num_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.08375632201903516, 'units': 97, 'learning_rate': 0.00016597222285836603}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:03,754] Trial 2 finished with value: 0.5436999797821045 and parameters: {'num_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.26771297084120765, 'units': 78, 'learning_rate': 3.996994844620452e-05}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:06,481] Trial 3 finished with value: 0.8906999826431274 and parameters: {'num_layers': 2, 'activation': 'sigmoid', 'dropout_rate': 0.35016726894861894, 'units': 79, 'learning_rate': 0.0002150544168986088}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:08,840] Trial 4 finished with value: 0.9343000054359436 and parameters: {'num_layers': 1, 'activation': 'relu', 'dropout_rate': 0.07603729660497355, 'units': 105, 'learning_rate': 0.0001392665961717344}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:11,739] Trial 5 finished with value: 0.8504999876022339 and parameters: {'num_layers': 3, 'activation': 'relu', 'dropout_rate': 0.12315683383336218, 'units': 74, 'learning_rate': 1.7798318517751306e-05}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:15,026] Trial 6 finished with value: 0.11580000072717667 and parameters: {'num_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.4371420318611797, 'units': 82, 'learning_rate': 1.2490451948092646e-05}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:17,748] Trial 7 finished with value: 0.11349999904632568 and parameters: {'num_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.050411055454690545, 'units': 66, 'learning_rate': 1.2805484121999906e-05}. Best is trial 0 with value: 0.9355000257492065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:20,935] Trial 8 finished with value: 0.9718000292778015 and parameters: {'num_layers': 2, 'activation': 'sigmoid', 'dropout_rate': 0.22610643884430093, 'units': 115, 'learning_rate': 0.006343055284400222}. Best is trial 8 with value: 0.9718000292778015.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:23,338] Trial 9 finished with value: 0.7124999761581421 and parameters: {'num_layers': 2, 'activation': 'sigmoid', 'dropout_rate': 0.024452745907070084, 'units': 65, 'learning_rate': 3.236651932520882e-05}. Best is trial 8 with value: 0.9718000292778015.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:26,788] Trial 10 finished with value: 0.9696000218391418 and parameters: {'num_layers': 2, 'activation': 'relu', 'dropout_rate': 0.2031224075587854, 'units': 125, 'learning_rate': 0.00681347001168451}. Best is trial 8 with value: 0.9718000292778015.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:30,134] Trial 11 finished with value: 0.9650999903678894 and parameters: {'num_layers': 2, 'activation': 'relu', 'dropout_rate': 0.2008748153697917, 'units': 126, 'learning_rate': 0.008882210008656525}. Best is trial 8 with value: 0.9718000292778015.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2024-11-18 16:37:32,147] Trial 12 finished with value: 0.9556000232696533 and parameters: {'num_layers': 2, 'activation': 'relu', 'dropout_rate': 0.22202269859855361, 'units': 37, 'learning_rate': 0.0072151581651029115}. Best is trial 8 with value: 0.9718000292778015.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[W 2024-11-18 16:37:32,461] Trial 13 failed with parameters: {'num_layers': 1, 'activation': 'relu', 'dropout_rate': 0.16047732134640716, 'units': 127, 'learning_rate': 0.0016765125062504528} because of the following error: InvalidArgumentError().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 28, in objective\n",
      "    history = model.fit(x_train, y_train,\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node Adam/mul_15 defined at (most recent call last):\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "\n",
      "  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 41, in <module>\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/study.py\", line 475, in optimize\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 28, in objective\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1783, in fit\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 241, in _update_step\n",
      "\n",
      "  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/adam.py\", line 204, in update_step\n",
      "\n",
      "Incompatible shapes: [10] vs. [0]\n",
      "\t [[{{node Adam/mul_15}}]] [Op:__inference_train_function_726029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-11-18 16:37:32,462] Trial 13 failed with value None.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node Adam/mul_15 defined at (most recent call last):\n  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 41, in <module>\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 28, in objective\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 241, in _update_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/adam.py\", line 204, in update_step\n\nIncompatible shapes: [10] vs. [0]\n\t [[{{node Adam/mul_15}}]] [Op:__inference_train_function_726029]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Create an Optuna study and optimize the objective function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Set the number of trials\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print the best trial\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[36], line 28\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[1;32m     24\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# To keep it quick for demonstration; you can increase it\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node Adam/mul_15 defined at (most recent call last):\n  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 41, in <module>\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"/var/folders/yk/83x2smm96b9_trlwmwsyv1g00000gn/T/ipykernel_55045/3145014312.py\", line 28, in objective\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 241, in _update_step\n\n  File \"/Users/conkey/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/adam.py\", line 204, in update_step\n\nIncompatible shapes: [10] vs. [0]\n\t [[{{node Adam/mul_15}}]] [Op:__inference_train_function_726029]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'sigmoid'])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    units = trial.suggest_int('units', 32, 128)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(units=units, activation=activation))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_split=0.2,\n",
    "                        epochs=5,  # To keep it quick for demonstration; you can increase it\n",
    "                        batch_size=128, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    accuracy = score[1]\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # Set the number of trials\n",
    "\n",
    "# Print the best trial\n",
    "print('Best trial:')\n",
    "print(f' Value: {study.best_trial.value}')\n",
    "print(' Params: ')\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
